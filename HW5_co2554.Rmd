---
title: "Data Science II Homework 5"
author: "Camille Okonkwo"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
editor_options: 
  chunk_output_type: console
--- 
\newpage

```{r setup, include=FALSE}
library(tidyverse)

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(tidymodels)
library(caret)
library(ISLR)
library(kernlab)
library(factoextra)
set.seed(2)
```
\newpage

# Question 1

## Background

In this problem, we will apply support vector machines to predict whether a given car gets high or low gas mileage based on the dataset `auto.csv` (used in Homework 3; see Homework 3 for more details of the dataset). The response variable is `mpg cat`. The predictors are `cylinders`, `displacement`, `horsepower`, `weight`, `acceleration`, `year`, and `origin`. Split the dataset into two parts: training data (70%) and test data (30%).

```{r partition}
auto = read_csv("data/auto.csv") |> 
  drop_na() |> 
  mutate(
    mpg_cat = as.factor(mpg_cat),
    mpg_cat = forcats::fct_relevel(mpg_cat, c("low", "high")),
    cylinders = as.factor(cylinders),
    origin = as.factor(origin)
  )

set.seed(2)

# create a random split of 70% training and 30% test data 
data_split = initial_split(data = auto, prop = 0.7)

# partitioned datasets
training_data = training(data_split)
testing_data = testing(data_split)

head(training_data)
head(testing_data)

# training data
x_1 = model.matrix(mpg_cat ~ ., training_data)[, -1] # matrix of predictors
head(x_1)
y_1 = training_data$mpg_cat # vector of response

# testing data
x_2 = model.matrix(mpg_cat ~ .,testing_data)[, -1] # matrix of predictors
y_2 = testing_data$mpg_cat # vector of response
```
\newpage

## (a) Fit a support vector classifier to the training data. What are the training and test error rates?
```{r}
ctrl = trainControl(method = "cv")
# kernlab
set.seed(2)
svml.fit = train(x_1, y_1,
                 data = training_data,
                 method = "svmLinear",
                 tuneGrid = data.frame(C = exp(seq(-5, 2, len = 50))),
                 trControl = ctrl)
plot(svml.fit, highlight = TRUE, xTrans = log)

# what are the training error rates?
svml.predict = predict(svml.fit,
              newdata = x_1)

confusionMatrix(data = svml.predict,
                reference = y_1)
# 1 - accuracy
svml_training_error = 1 - 0.9492
svml_training_error

# test error
svml.test = predict(svml.fit,
              newdata = x_2)

confusionMatrix(data = svml.test,
                reference = y_2)
# 1 - accuracy
svml_test_error = 1 - 0.9492
svml_test_error
```
\newpage

## (b) Fit a support vector machine with a radial kernel to the training data. What are the training and test error rates?
```{r}
svmr.grid = expand.grid(C = exp(seq(1, 7, len = 50)),
                        sigma = exp(seq(-10, -2, len = 20))) # how to tune?

# SVM with radial kernel
set.seed(2)
svmr.fit = train(x_1, y_1, 
                 data = training_data,
                 method = "svmRadialSigma",
                 tuneGrid = svmr.grid,
                 trControl = ctrl)
svmr.fit$bestTune
myCol = rainbow(25)
myPar = list(superpose.symbol = list(col = myCol),
             superpose.line = list(col = myCol))

plot(svmr.fit, highlight = TRUE, par.settings = myPar)

# training error
svmr.predict = predict(svmr.fit,
              newdata = x_1)

confusionMatrix(data = svmr.predict,
                reference = y_1)
# 1 - accuracy
svmr_training_error = 1 - 0.938
svmr_training_error

# test error
svmr.test = predict(svmr.fit,
              newdata = x_2)

confusionMatrix(data = svmr.test,
                reference = y_2)
# 1 - accuracy
svmr_test_error = 1 - 0.9576
svmr_test_error


```
\newpage

# Question 2

## Background

In this problem, we perform hierarchical clustering on the states using the `USArrests` data in the `ISLR` package. For each of the 50 states in the United States, the dataset contains the number of arrests per 100,000 residents for each of three crimes: `Assault`, `Murder`, and `Rape`. The dataset also contains the percent of the population in each state living in urban areas, `UrbanPop`. The four variables will be used as features for clustering.

```{r}
data(USArrests)

us_arrests = na.omit(USArrests)
```
\newpage

## (a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states. Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?
```{r}
# complete linkage and euclidean distance
hc.complete = hclust(dist(us_arrests), method = "complete")

# hierarchical clustering 
fviz_dend(hc.complete, k = 3,
          cex = 0.3,
          palette = "jco",
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

us.complete = cutree(hc.complete, 3)

# 1st cluster states
us_arrests[us.complete == 1,]

# 2nd cluster states
us_arrests[us.complete == 2,]

# 3rd cluster states
us_arrests[us.complete == 3,]
```
\newpage

## (b) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.
```{r}
# scale the variables to have standard deviation one
scaled_arrests = scale(us_arrests)

# complete linkage and Euclidean distance
hc_complete = hclust(dist(scaled_arrests), method = "complete")

# hierarchial clustering dendogram
fviz_dend(hc_complete, k = 3,
          cex = 0.3,
          palette = "jco",
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

clusters_scaled = cutree(hc_complete, k = 3)

# 1st cluster states
us_arrests[clusters_scaled == 1,]

# 2nd cluster states
us_arrests[clusters_scaled == 2,]

# 3rd cluster states
us_arrests[clusters_scaled == 3,]
```
\newpage

## (e) Does scaling the variables change the clustering results? Why? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed?

Scaling the variables before hierarchical clustering does affect the clustering results. Hierarchical clustering and methods like complete linkage and Euclidean distance are sensitive to the scale of variables. When variables are on different scales or have different variances, those with larger variances can dominate the distance computations. This can lead to clusters being formed primarily based on the variables with larger scales or variances, rather than considering the overall patterns across all variables equally.In my opinion, scaling the variables before computing inter-observation dissimilarities can be beneficial. It helps to ensure that all variables contribute equally to the distance computations, preventing dominance by variables with larger scales. This can lead to more balanced and meaningful clusters that capture the underlying patterns in the data more accurately.However, it's important to keep in mind there are certain situations scaling is necessary and scaling is not necessary. For example, if the variables are already on similar scales or if the specific research question or domain knowledge suggests that variable scaling should not be applied, then clustering without scaling might be more appropriate.







