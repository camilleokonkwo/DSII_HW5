---
title: "Data Science II Homework 5"
author: "Camille Okonkwo"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
editor_options: 
  chunk_output_type: console
--- 
\newpage

```{r setup, include=FALSE}
library(tidyverse)

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(tidymodels)
library(caret)
library(ISLR)
library(kernlab)
library(factoextra)
set.seed(2)
```
\newpage

# Question 1

## Background

In this problem, we will apply support vector machines to predict whether a given car gets high or low gas mileage based on the dataset `auto.csv` (used in Homework 3; see Homework 3 for more details of the dataset). The response variable is `mpg cat`. The predictors are `cylinders`, `displacement`, `horsepower`, `weight`, `acceleration`, `year`, and `origin`. Split the dataset into two parts: training data (70%) and test data (30%).

```{r Q1_partition}
auto = read_csv("data/auto.csv") |> 
  drop_na() |> 
  mutate(
    mpg_cat = as.factor(mpg_cat),
    mpg_cat = forcats::fct_relevel(mpg_cat, c("low", "high")),
    cylinders = as.factor(cylinders),
    origin = as.factor(origin)
  )

set.seed(2)

# create a random split of 70% training and 30% test data 
data_split = initial_split(data = auto, prop = 0.7)

# partitioned datasets
training_data = training(data_split)
testing_data = testing(data_split)

head(training_data)
head(testing_data)

# training data
x_1 = model.matrix(mpg_cat ~ ., training_data)[, -1] # matrix of predictors
head(x_1)
y_1 = training_data$mpg_cat # vector of response

# testing data
x_2 = model.matrix(mpg_cat ~ .,testing_data)[, -1] # matrix of predictors
y_2 = testing_data$mpg_cat # vector of response
```
\newpage

## (a) Fit a support vector classifier to the training data. What are the training and test error rates?
```{r Q_1a}
# 10-fold cross-validation
ctrl = trainControl(method = "cv", number = 10)


set.seed(2)

# support vector classifier
svml.fit = train(x_1, y_1,
                 method = "svmLinear",
                 tuneGrid = data.frame(C = exp(seq(-1, 8, len = 50))),
                 trControl = ctrl)

plot(svml.fit, highlight = TRUE, xTrans = log)

svml.fit$bestTune

# what are the training error rates?
svml.predict = predict(svml.fit,
              newdata = x_1)

confusionMatrix(data = svml.predict,
                reference = y_1,
                )
# 1 - accuracy
svml_training_error = 1 - 0.9234
svml_training_error

# test error
svml.test = predict(svml.fit,
              newdata = x_2)

confusionMatrix(data = svml.test,
                reference = y_2,
                )
# 1 - accuracy
svml_test_error = 1 - 0.9492
svml_test_error
```

The training error rate is `r svml_training_error`, or `r svml_training_error * 100`%. The testing error rate is `r svml_test_error`, or `r svml_test_error * 100`%.

\newpage

## (b) Fit a support vector machine with a radial kernel to the training data. What are the training and test error rates?
```{r Q_1b}
svmr.grid = expand.grid(C = exp(seq(1, 5, len = 50)),
                        sigma = exp(seq(-8, 1, len = 20)))

set.seed(2)

# SVM with radial kernel
svmr.fit = train(x_1, y_1, 
                 method = "svmRadialSigma",
                 tuneGrid = svmr.grid,
                 trControl = ctrl)

svmr.fit$bestTune
myCol = rainbow(25)
myPar = list(superpose.symbol = list(col = myCol),
             superpose.line = list(col = myCol))

plot(svmr.fit, highlight = TRUE, par.settings = myPar)

# training error
svmr.predict = predict(svmr.fit,
              newdata = x_1)

confusionMatrix(data = svmr.predict,
                reference = y_1,
                )
# 1 - accuracy
svmr_training_error = 1 - 0.9416
svmr_training_error

# test error
svmr.test = predict(svmr.fit,
              newdata = x_2)

confusionMatrix(data = svmr.test,
                reference = y_2,
                )
# 1 - accuracy
svmr_test_error = 1 - 0.9576
svmr_test_error
```

The training error rate is `r svmr_training_error`, or `r svmr_training_error * 100`%. The testing error rate is `r svmr_test_error`, or `r svmr_test_error * 100`%.
\newpage

# Question 2

## Background

In this problem, we perform hierarchical clustering on the states using the `USArrests` data in the `ISLR` package. For each of the 50 states in the United States, the dataset contains the number of arrests per 100,000 residents for each of three crimes: `Assault`, `Murder`, and `Rape`. The dataset also contains the percent of the population in each state living in urban areas, `UrbanPop`. The four variables will be used as features for clustering.

```{r Q2_data }
data(USArrests)

us_arrests = na.omit(USArrests)
```
\newpage

## (a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states. Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?
```{r Q_2a}
# complete linkage and euclidean distance
hc.complete = hclust(dist(us_arrests), method = "complete")

# hierarchical clustering 
fviz_dend(hc.complete, k = 3,
          cex = 0.3,
          palette = "jco",
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

us.complete = cutree(hc.complete, 3)

# 1st cluster states
complete_1 = us_arrests[us.complete == 1,]
complete_1
state_names1 = rownames(complete_1)

# 2nd cluster states
complete_2 = us_arrests[us.complete == 2,]
complete_2
state_names2 = rownames(complete_2)

# 3rd cluster states
complete_3 = us_arrests[us.complete == 3,]
complete_3
state_names3 = rownames(complete_3)
```

The states that belong to cluster one are `r state_names1`. The states that belong to cluster two are `r state_names2`. The states that belong to cluster three are `r state_names3`. 
\newpage

## (b) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.
```{r Q_2b}
# scale the variables to have standard deviation one
scaled_arrests = scale(us_arrests)

# complete linkage and Euclidean distance
hc_complete = hclust(dist(scaled_arrests), method = "complete")

# hierarchical clustering dendogram
fviz_dend(hc_complete, k = 3,
          cex = 0.3,
          palette = "jco",
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

clusters_scaled = cutree(hc_complete, k = 3)

# 1st cluster states
scaled_1 = us_arrests[clusters_scaled == 1,]
scaled_1
scaled_names1 = rownames(scaled_1)

# 2nd cluster states
scaled_2 = us_arrests[clusters_scaled == 2,]
scaled_2
scaled_names2 = rownames(scaled_2)

# 3rd cluster states
scaled_3 = us_arrests[clusters_scaled == 3,]
scaled_3
scaled_names3 = rownames(scaled_3)
```
\newpage

## (e) Does scaling the variables change the clustering results? Why? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed?

Scaling the variables does change the clustering results. This is because hierarchical clustering methods, specifically methods like complete linkage and Euclidean distance, are sensitive to variable scales and variances. When variables have different scales or variances, those with larger variations can disproportionately influence the distance calculations, potentially leading to clusters that are biased towards these variables.

In my opinion, scaling the variables before computing inter-observation dissimilarities is advantageous. It ensures that each variable contributes equally to the distance calculations, preventing any single variable from dominating the clustering process. This approach typically results in more balanced clusters that better capture the underlying patterns in the data.

It's important to consider, however, the situations where scaling may not be necessary or even counterproductive. For example, if the variables are already on similar scales or if domain knowledge/the research question suggests that scaling is unnecessary for the specific analysis, then clustering without scaling might be more appropriate.